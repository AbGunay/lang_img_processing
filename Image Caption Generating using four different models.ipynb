{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1pGiuVA4DId"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding,BatchNormalization, Dropout, Input, TimeDistributed, Dense, add, Merge, RepeatVector, Activation, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "from keras import backend \n",
    "from keras.models import load_model\n",
    "import time\n",
    "from PIL import Image\n",
    "from keras.utils import plot_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmJ7Ajya4DI1"
   },
   "outputs": [],
   "source": [
    "# code to make caption dictionary whose keys are image file name and values are image caption.\n",
    "token_dir = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "\n",
    "image_captions = open(token_dir).read().split('\\n')\n",
    "caption = {}    \n",
    "for i in range(len(image_captions)-1):\n",
    "    id_capt = image_captions[i].split(\"\\t\")\n",
    "    id_capt[0] = id_capt[0][:len(id_capt[0])-2] # to rip off the #0,#1,#2,#3,#4 from the tokens file\n",
    "    if id_capt[0] in caption:\n",
    "        caption[id_capt[0]].append(id_capt[1])\n",
    "    else:\n",
    "        caption[id_capt[0]] = [id_capt[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3> Two files were made that named \"trainimgs.txt\" and \"testImages.txt\" that will have start and end token at the start and end of each caption respectively.  </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1203,
     "status": "error",
     "timestamp": 1535333439723,
     "user": {
      "displayName": "Faizan E Mustafa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104300788297321667476"
     },
     "user_tz": 0
    },
    "id": "f_XBL4iQ4DJf",
    "outputId": "ddb1b071-2141-4f22-f25b-9d0a7e821820"
   },
   "outputs": [],
   "source": [
    "train_imgs_id = open(\"Flickr8k_text/Flickr_8k.trainImages.txt\").read().split('\\n')[:-1]\n",
    "train_imgs_captions = open(\"Flickr8k_text/trainimgs.txt\",'w')\n",
    "for img_id in train_imgs_id:\n",
    "    for captions in caption[img_id]:\n",
    "        desc = \"<start> \"+captions+\" <end>\"\n",
    "        train_imgs_captions.write(img_id+\"\\t\"+desc+\"\\n\")\n",
    "        train_imgs_captions.flush()\n",
    "train_imgs_captions.close()\n",
    "\n",
    "test_imgs_id = open(\"Flickr8k_text/Flickr_8k.testImages.txt\").read().split('\\n')[:-1]\n",
    "test_imgs_captions = open(\"Flickr8k_text/testimgs.txt\",'w')\n",
    "for img_id in test_imgs_id:\n",
    "    for captions in caption[img_id]:\n",
    "        desc = \"<start> \"+captions+\" <end>\"\n",
    "        test_imgs_captions.write(img_id+\"\\t\"+desc+\"\\n\")\n",
    "        test_imgs_captions.flush()\n",
    "test_imgs_captions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_captions = open(\"Flickr8k_text/testimgs.txt\").read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDuUDBE-4DJr"
   },
   "outputs": [],
   "source": [
    "#normalize the image\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AfWZaLQs4DJ3"
   },
   "outputs": [],
   "source": [
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We are using VGG model to extract features from images. We will only change the output layer of the model, now our output will be second last layer of the model which gives output (4096,) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6US8C2V4DKQ"
   },
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "model.layers.pop()\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Encode function takes in image path and outputs the vector using the VGG model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMk9PUiF4DKc"
   },
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image)\n",
    "    temp_enc = model.predict(image)\n",
    "    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n",
    "    return temp_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWBBl1vK4DK1"
   },
   "outputs": [],
   "source": [
    "images = 'Flickr8k_Dataset/Flicker8k_Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6FRS9YR4DLP",
    "outputId": "10fc72fe-2ddb-4cc2-82ac-b2cd7aed2c6b"
   },
   "outputs": [],
   "source": [
    "train_imgs_id = open(\"Flickr8k_text/Flickr_8k.trainImages.txt\").read().split('\\n')[:-1]\n",
    "test_imgs_id = open(\"Flickr8k_text/Flickr_8k.testImages.txt\").read().split('\\n')[:-1]\n",
    "encoding_train = {}\n",
    "for img in tqdm(train_imgs_id): #tqdm instantly make your loops show a smart progress meter\n",
    "    path = images+str(img)\n",
    "    encoding_train[img] = encode(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bwciSYoj4DLh"
   },
   "outputs": [],
   "source": [
    "with open(\"encoded_train_images_vgg.p\", \"wb\") as encoded_pickle: \n",
    "    pickle.dump(encoding_train, encoded_pickle) #python object can be pickled so that it can be saved on disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbiBjJ-f4DLt"
   },
   "outputs": [],
   "source": [
    "encoding_train = pickle.load(open('encoded_train_images_vgg.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1535342919790,
     "user": {
      "displayName": "Faizan E Mustafa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104300788297321667476"
     },
     "user_tz": 0
    },
    "id": "0yY_qzKY4DL3",
    "outputId": "9ac68250-cbdf-4220-808d-f4f5692884c4"
   },
   "outputs": [],
   "source": [
    "encoding_train['3556792157_d09d42bef7.jpg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NvZb9GZ4DMF",
    "outputId": "5484e5f5-cb99-46e3-deb8-e1c6b031c65e"
   },
   "outputs": [],
   "source": [
    "encoding_test = {}\n",
    "for img in tqdm(test_imgs_id):\n",
    "    path = images+str(img)\n",
    "    encoding_test[img] = encode(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vA-V2fu4DMT"
   },
   "outputs": [],
   "source": [
    "with open(\"encoded_test_images_vgg.p\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding_test, encoded_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJyUZflC4DMf"
   },
   "outputs": [],
   "source": [
    "encoding_test = pickle.load(open('encoded_test_images_vgg.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Flickr8k_text/trainimgs.txt', delimiter='\\t')\n",
    "captionz = []\n",
    "img_id = []\n",
    "dataframe = dataframe.sample(frac=1)\n",
    "iter = dataframe.iterrows()\n",
    "\n",
    "for i in range(len(dataframe)):\n",
    "    nextiter = next(iter)\n",
    "    captionz.append(nextiter[1][1])\n",
    "    img_id.append(nextiter[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the number of the samples\n",
    "no_samples=0\n",
    "tokens = []\n",
    "tokens = [i.split() for i in captionz]\n",
    "for caption in captionz:\n",
    "    no_samples+=len(caption.split())-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Flickr8k_text/testimgs.txt', delimiter='\\t')\n",
    "test_captionz = []\n",
    "test_img_id = []\n",
    "dataframe = dataframe.sample(frac=1)\n",
    "iter = dataframe.iterrows()\n",
    "\n",
    "for i in range(len(dataframe)):\n",
    "    nextiter = next(iter)\n",
    "    test_captionz.append(nextiter[1][1])\n",
    "    test_img_id.append(nextiter[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating vocabulary \n",
    "vocab= [] \n",
    "for token in tokens:\n",
    "    vocab.extend(token)\n",
    "vocab = list(set(vocab))\n",
    "with open(\"vocab.p\", \"wb\") as pickle_d:\n",
    "    pickle.dump(vocab, pickle_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab= pickle.load(open('vocab.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3976,
     "status": "ok",
     "timestamp": 1535342927926,
     "user": {
      "displayName": "Faizan E Mustafa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104300788297321667476"
     },
     "user_tz": 0
    },
    "id": "_K1ahe9n4DM_",
    "outputId": "2ab41fb4-9752-494a-e29e-38c8a62c825d"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6PQ8TMy4DOJ"
   },
   "outputs": [],
   "source": [
    "#tokenize\n",
    "word_idx = {val:index for index, val in enumerate(vocab)}\n",
    "idx_word = {index:val for index, val in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1535342934922,
     "user": {
      "displayName": "Faizan E Mustafa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104300788297321667476"
     },
     "user_tz": 0
    },
    "id": "W7GMqJw14DOW",
    "outputId": "bfa057ff-ff65-46ce-fd21-cae2e8e2b3a5"
   },
   "outputs": [],
   "source": [
    "#calculate the maxlength sentence to padd the samples for training\n",
    "caption_length = [len(caption.split()) for caption in captionz]\n",
    "max_length = max(caption_length)\n",
    "max_length # maximum lenght of a caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKtQDylY4DOi"
   },
   "outputs": [],
   "source": [
    "def data_process(captionz, encoding_train, word_idx, batch_size, max_length):\n",
    "    partial_captions = []\n",
    "    next_words = []\n",
    "    images = []\n",
    "    total_count = 0\n",
    "    while 1:\n",
    "        for image_counter, caption in enumerate(captionz):\n",
    "            current_image = encoding_train[img_id[image_counter]]\n",
    "    \n",
    "            for i in range(len(caption.split())-1):\n",
    "                total_count+=1\n",
    "                partial = [word_idx[txt] for txt in caption.split()[:i+1]]\n",
    "                partial_captions.append(partial)\n",
    "                next = np.zeros(vocab_size)\n",
    "                next[word_idx[caption.split()[i+1]]] = 1\n",
    "                next_words.append(next)\n",
    "                images.append(current_image)\n",
    "                if total_count>=batch_size:\n",
    "                    next_words = np.asarray(next_words)\n",
    "                    images = np.asarray(images)\n",
    "                    partial_captions = sequence.pad_sequences(partial_captions, maxlen=max_length, padding='post')\n",
    "                    total_count = 0\n",
    "                    yield [[images, partial_captions], next_words]\n",
    "                    partial_captions = []\n",
    "                    next_words = []\n",
    "                    images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_par_inject(captionz, encoding_train, word_idx, batch_size, max_length):\n",
    "    partial_captions = []\n",
    "    next_words = []\n",
    "    images = []\n",
    "    total_count = 0\n",
    "    while 1:\n",
    "    \n",
    "        for image_counter, caption in enumerate(captionz):\n",
    "            current_image = encoding_train[img_id[image_counter]].tolist()\n",
    "            for i in range(len(caption.split())-1):\n",
    "                total_count+=1\n",
    "                partial = [word_idx[txt] for txt in caption.split()[:i+1]]\n",
    "                partial_captions.append(current_image + partial)\n",
    "                next = np.zeros(vocab_size)\n",
    "                next[word_idx[caption.split()[i+1]]] = 1\n",
    "                next_words.append(next)\n",
    "                #images.append(current_image)\n",
    "\n",
    "                if total_count>=batch_size:\n",
    "                    next_words = np.asarray(next_words)\n",
    "                    images = np.asarray(images)\n",
    "                    partial_captions = sequence.pad_sequences(partial_captions, maxlen=4096+max_length, padding='post')\n",
    "                    total_count = 0\n",
    "                \n",
    "                    yield [partial_captions, next_words]\n",
    "                    partial_captions = []\n",
    "                    next_words = []\n",
    "                    images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_pre_inject(captionz, encoding_train, word_idx, batch_size):\n",
    "    partial_captions = []\n",
    "    next_words = []\n",
    "    images = []\n",
    "    total_count = 0\n",
    "    while 1:\n",
    "    \n",
    "        for image_counter, caption in enumerate(captionz):\n",
    "            current_image = encoding_train[img_id[image_counter]]\n",
    "            for i in range(len(caption.split())-1):\n",
    "                total_count+=1\n",
    "                partial = [word_idx[txt] for txt in caption.split()[:i+1]]\n",
    "                partial_captions.append(partial)\n",
    "                next = np.zeros(vocab_size)\n",
    "                next[word_idx[caption.split()[i+1]]] = 1\n",
    "                next_words.append(next)\n",
    "                images.append(current_image)\n",
    "\n",
    "                if total_count>=batch_size:\n",
    "                    next_words = np.asarray(next_words)\n",
    "                    images = np.asarray(images)\n",
    "                    partial_captions = sequence.pad_sequences(partial_captions, maxlen=4096, padding='post')\n",
    "                    total_count = 0\n",
    "                    yield [[images, partial_captions], next_words]\n",
    "                    partial_captions = []\n",
    "                    next_words = []\n",
    "                    images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9EAGj_o14DOn"
   },
   "outputs": [],
   "source": [
    "#pre-inject\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dense(128, activation='relu')(inputs1)\n",
    "# sequence model\n",
    "inputs2 = Input(shape=(4096,))\n",
    "se1 = Embedding(vocab_size, 128, mask_zero=True)(inputs2)\n",
    "inputs = add([fe1, se1])\n",
    "se2 = LSTM(128)(inputs)\n",
    "se3 = Dropout(0.5)(se2)\n",
    "# decoder model\n",
    "decoder1 = Dense(64, activation='relu')(se3)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
    "# tie it together [image, seq] [word]\n",
    "pre_inject_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "plot_model(pre_inject_model, to_file='pre_inject_model.png', show_shapes=True)\n",
    "pre_inject_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epoch=1\n",
    "pre_inject_model.fit_generator(data_process_pre_inject(captionz, encoding_train, word_idx, batch_size), \n",
    "                               steps_per_epoch=no_samples/batch_size, epochs=epoch, verbose=1, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8717,
     "status": "ok",
     "timestamp": 1535337275801,
     "user": {
      "displayName": "Faizan E Mustafa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104300788297321667476"
     },
     "user_tz": 0
    },
    "id": "DI_P9-1J4DOs",
    "outputId": "0bab4a6b-d1a8-4911-f585-815039928c36"
   },
   "outputs": [],
   "source": [
    "#merge model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dense(128, activation='relu')(inputs1)\n",
    "# sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 128, mask_zero=True)(inputs2)\n",
    "se2 = LSTM(128)(se1)\n",
    "se3 = Dropout(0.5)(se2)\n",
    "# decoder model\n",
    "decoder1 = add([fe1, se3])\n",
    "decoder2 = Dense(64, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "# tie it together [image, seq] [word]\n",
    "merge_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "plot_model(merge_model, to_file='merge_model.png', show_shapes=True)\n",
    "merge_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_model.fit_generator(data_process(captionz, encoding_train, word_idx, batch_size, max_length), \n",
    "                               steps_per_epoch=no_samples/batch_size, epochs=epoch, verbose=1, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init_inject\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(128, activation='relu')(fe1)\n",
    "# sequence model\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 128, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(128)(se2, initial_state=[fe2, fe2])\n",
    "# decoder model\n",
    "#decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(128, activation='relu')(se3)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "# tie it together [image, seq] [word]\n",
    "init_inject_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "plot_model(init_inject_model, to_file='init_inject_model.png', show_shapes=True)\n",
    "init_inject_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_inject_model.fit_generator(data_process(captionz, encoding_train, word_idx, batch_size, max_length), \n",
    "                               steps_per_epoch=no_samples/batch_size, epochs=epoch, verbose=1, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par_inject model\n",
    "inputs1 = Input(shape=(4096+max_length,))\n",
    "se1 = Embedding(vocab_size, 128, mask_zero=True)(inputs1)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(128)(se2)\n",
    "# decoder model\n",
    "#decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(64, activation='relu')(se3)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "# tie it together [image, seq] [word]\n",
    "par_inject_model = Model(inputs= inputs1, outputs=outputs)\n",
    "plot_model(par_inject_model, to_file='par_inject_model.png', show_shapes=True)\n",
    "par_inject_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6O84NWs4DO7"
   },
   "outputs": [],
   "source": [
    "par_inject_model.fit_generator(data_process_par_inject(captionz, encoding_train, word_idx, batch_size, max_length), \n",
    "                               steps_per_epoch=no_samples/batch_size, epochs=epoch, verbose=1, callbacks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGyZDiME4DQT"
   },
   "source": [
    "<h3>In order to predict  results I used  greedy search. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYmVVbiI4DQc"
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(test_imgs_id, test_captionz, word_idx, idx_word, sequence, max_length, encoding_test, model, method = \"not par\"):\n",
    "    actual_caption = []\n",
    "    predicted_caption = []\n",
    "    for encounter, caption in enumerate(test_captionz):\n",
    "        #print(predict_captions(test_img_id[encounter], word_idx, idx_word, sequence, max_length, encoding_test, model))\n",
    "        #print(caption.split()[1:-1])\n",
    "        prediction = predict_captions(test_img_id[encounter], word_idx, idx_word, sequence, max_length, encoding_test, model, method).split()\n",
    "        predicted_caption.append(prediction)\n",
    "        actual_caption.append([caption.split()[1:-1]])\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual_caption, predicted_caption, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual_caption, predicted_caption, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual_caption, predicted_caption, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual_caption, predicted_caption, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2956,
     "status": "ok",
     "timestamp": 1535342849757,
     "user": {
      "displayName": "Faizan E Mustafa",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "104300788297321667476"
     },
     "user_tz": 0
    },
    "id": "ovsWyxla7C_0",
    "outputId": "f1a8fa9b-040a-43c1-d83f-b7bc59edcdc1"
   },
   "outputs": [],
   "source": [
    "def predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, model, method = 'not par'):\n",
    "    start_word = [\"<start>\"]\n",
    "    while 1:\n",
    "        now_caps = [word_idx[i] for i in start_word]\n",
    "        now_caps = sequence.pad_sequences([now_caps], maxlen=max_length, padding='post')\n",
    "        e = encoding_test[image_file]\n",
    "        if method == 'not par':\n",
    "            preds = model.predict([np.array([e]), np.array(now_caps)])\n",
    "        else:\n",
    "            _input = e.tolist() + now_caps[0].tolist()\n",
    "            preds = model.predict(np.array([_input]))\n",
    "        word_pred = idx_word[np.argmax(preds[0])]\n",
    "        start_word.append(word_pred)\n",
    "        \n",
    "        if word_pred == \"<end>\" or len(start_word) > max_length: \n",
    "            break            \n",
    "    return ' '.join(start_word[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating int_inject_model\n",
    "evaluate_model(test_img_id, test_captionz, word_idx, idx_word, sequence, max_length, encoding_test, init_inject_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating pre_inject_model\n",
    "evaluate_model(test_img_id, test_captionz, word_idx, idx_word, sequence, max_length, encoding_test, pre_inject_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating par_inject_model\n",
    "evaluate_model(test_img_id, test_captionz, word_idx, idx_word, sequence, max_length, encoding_test, par_inject_model, method='par')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating merge model\n",
    "evaluate_model(test_img_id, test_captionz, word_idx, idx_word, sequence, max_length, encoding_test, merge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the merge model outperform others in the bleu score results, I trained it for 5 epochs to get reasonable predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=5\n",
    "merge_model.fit_generator(data_process(captionz, encoding_train, word_idx, batch_size, max_length), \n",
    "                               steps_per_epoch=no_samples/batch_size, epochs=epoch, verbose=1, callbacks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"3430607596_7e4f74e3ff.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"3255482333_5bcee79f7e.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"3168123064_d1983b8f92.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"3316725440_9ccd9b5417.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"3218480482_66af7587c8.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"2541104331_a2d65cfa54.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"309687244_4bdf3b591f.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"2542662402_d781dd7f7c.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file =\"2654514044_a70a6e2c21.jpg\"\n",
    "test_image =  images + image_file\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print original five texts for the image\n",
    "image_index = [i for i in range(len(test_img_id)) if test_img_id[i] == image_file]\n",
    "print('Original five sentences')\n",
    "print(test_captionz[image_index[0]])\n",
    "print(test_captionz[image_index[1]])\n",
    "print(test_captionz[image_index[2]])\n",
    "print(test_captionz[image_index[3]])\n",
    "print(test_captionz[image_index[4]])\n",
    "#print prediction of the merge model\n",
    "print ('Greedy search prediction:', predict_captions(image_file, word_idx, idx_word, sequence, max_length, encoding_test, merge_model))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My_code.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
